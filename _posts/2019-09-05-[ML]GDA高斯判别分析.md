---
layout: post
title: ^T[ML] GDA 高斯判别分析^T
subtitle: ^T原理与公式推导^T
date: 2019-09-05
categories: ML
cover: ^T/assets/image/GDA-header.png^T
tags: math MachineLearning
---

虽然逻辑回归在机器学习任务中的效果非常好，但在样本呈现特殊分布的情况下，我们可以使用其他更好的算法。**高斯判别分析 (GDA)** 就是其中的一个。这篇博客的主要内容，就是介绍高斯判别分析算法的主要原理以及公式的推导。

### 1. 高斯判别分析的先验假设

与逻辑回归不同，高斯判别分析需要两个先验假设，分别为：

-   样本的分类 $y$ 服从伯努利分布：

$$
\begin{aligned}
p(y) =
\left\{
\begin{aligned}
&\phi^y (1 - \phi)^{1 - y} && y = 0, 1 \\
& 0 && y \neq 0, 1
\end{aligned}
\right.
\end{aligned}
$$

-   正负样本均符合正态分布：

$$
\begin{aligned}
p(x|y = 0) &= \frac{1}{(2\pi)^{\frac n2} |\Sigma|^{\frac12}} \exp(-\frac12 (x-\mu_0)^T \Sigma^{-1} (x-\mu_0)) \\
p(x|y = 1) &= \frac{1}{(2\pi)^{\frac n2} |\Sigma|^{\frac12}} \exp(-\frac12 (x-\mu_1)^T \Sigma^{-1} (x-\mu_1))
\end{aligned}
$$

正因为在模型中，我们需要预先假设样本服从正态分布，所以这也是“高斯判别分析”名字的由来。

有了以上的假设之后，我们就能进行下一步的推导。

### 2. 高斯判别分析的参数估计

在前面的先验假设中，我们需要用到 $\phi$, $\Sigma$, $\mu_0$ 和 $\mu_1$ 等参数，所以我们先要对这些参数给出参数估计。

首先我们求出对数似然函数：

$$
\begin{aligned}
L(\phi,\Sigma,\mu_0,\mu_1)
&= \sum _{i = 1} ^m(\log p(x^{(i)}|y^{(i)}) + \log p(y^{(i)}))\\
&= \sum _{i = 1} ^m y^{(i)} \log p(x^{(i)}|y^{(i)}=1) + \sum _{i = 1} ^m (1 - y^{(i)}) \log p(x^{(i)}|y^{(i)} = 0) + \sum _{i = 1} ^m \log p(y^{(i)}
\end{aligned}
$$

然后，我们需要分别对 $\phi$, $\Sigma$, $\mu_0$, $\mu_1$ 求偏导，求出驻点。

首先先对 $\phi$ 求偏导，其中前两项和对 $\phi$ 的偏导均为 $0$ ，所以只需计算第三项的结果：

$$
\begin{aligned}
\frac{\partial L(\phi, \Sigma, \mu_0, \mu_1)}{\partial \phi}
&= \frac{\partial \sum _{i = 1} ^m \log p(y^{(i)})} {\partial \phi} \\
&= \frac{\partial \sum _{i = 1} ^m \log \phi^{y^{(i)}} (1 - \phi)^{1 - y^{(i)}}} {\partial \phi} \\
&= \frac{\partial \sum _{i = 1} ^m y^{(i)} \log \phi + (1 - y^{(i)}) \log (1 - \phi)} {\partial \phi} \\
&= \sum _{i = 1} ^m y^{(i)} \frac1\phi - (1 - y^{(i)}) \frac1{1 - \phi}
\end{aligned}
$$

令导数等于零，解得：

$$
\begin{aligned}
\phi
= \frac1m \sum _{i = 1} ^m \frac{y^{(i)}}{y^{(i)} + (1 - y^{(i)})}
= \frac1m \sum _{i = 1} ^m y^{(i)}
\end{aligned}
$$

$\Sigma$ 的求导过程较为复杂：

$$
\begin{aligned}
\frac{\partial L(\phi, \Sigma, \mu_0, \mu_1)}{\partial \Sigma}
&= \frac{\partial \sum _{i = 1} ^m y^{(i)} \log p(x^{(i)}|y^{(i)} = 1) + \sum _{i = 1} ^m (1 - y^{(i)}) \log p(x^{(i)}|y^{(i)} = 0)}{\partial \Sigma} \\
&= \frac{\partial \sum _{i = 1} ^m \log \frac1{(2 \pi)^{\frac n2} |\Sigma|^{\frac12}} - \frac12 \sum _{i = 1} ^m (x^{(i)} - \mu_{y^{(i)}})^T \Sigma^{-1} (x^{(i)} - \mu_{y^{(i)}})} {\partial \Sigma} \\
&= \frac{\partial - \frac m2 (n \log 2\pi + \log |\Sigma|) - \frac12 \sum _{i = 1} ^m (x^{(i)} - \mu_{y^{(i)}})^T \Sigma^{-1} (x^{(i)} - \mu_{y^{(i)}})}{\partial \Sigma} \\
&= -\frac m2 \Sigma^{-1} - \frac12 \sum _{i = 1} ^m (x^{(i)} - \mu_{y^{(i)}}) (x^{(i)} - \mu_{y^{(i)}})^T (\Sigma^{-1})^2
\end{aligned}
$$

令等式为 $0$ ，并右乘 $\Sigma^2$ ，解得的结果为：

$$
\begin{aligned}
\Sigma = \frac1m \sum _{i = 1} ^m (x^{(i)} - \mu_{y^{(i)}}) (x^{(i)} - \mu_{y^{(i)}})^T
\end{aligned}
$$

其中， $\Sigma$ 的表达式中会使用到 $\mu_0$, $\mu_1$ 的值，所以接下来需要求 $\mu_1$ 的似然估计：

$$
\begin{aligned}
\frac{\partial L(\phi, \Sigma, \mu_0, \mu_1)}{\partial \mu_1}
&= \frac{\partial \sum _{i = 1} ^m y^{(i)} \log p(x^{(i)}|y^{(i)} = 1)} {\partial \mu_1} \\
&= \frac{\partial \sum _{i = 1} ^m y^{(i)} \log \frac{1}{(2\pi)^{\frac n2} |\Sigma|^{\frac12}} \exp(-\frac12 (x-\mu_1)^T \Sigma^{-1} (x - \mu_1))}{\partial \mu_1} \\
&= \sum _{i = 1} ^m y^{(i)}\Sigma^{-1}(x^{(i)} - \mu_1)
\end{aligned}
$$

令导数为零，解得：

$$
\begin{aligned}
\mu_1 = \frac{\sum _{i = 1} ^m y^{(i)} x^{(i)}}{\sum _{i = 1} ^m y^{(i)}}
\end{aligned}
$$

同理可得：

$$
\begin{aligned}
\mu_0 = \frac{\sum _{i = 1} ^m (1 - y^{(i)}) x^{(i)}}{\sum _{i = 1} ^m (1 - y^{(i)})}
\end{aligned}
$$

由此我们就得到了所有参数的似然估计结果。

### 3. 高斯判别分析与逻辑回归的关系

首先需要求高斯判别分析的后验分布。由贝叶斯公式，我们可以将其转化为先验分布的形式：

$$
\begin{aligned}
p(y = 1|x)
&= \frac{p(x|y = 1) p(y = 1)}{p(x|y = 1) p(y = 1) + p(x|y = 0) p(y = 0)} \\
&= \frac{1}{1 + \frac{p(x|y = 0)}{p(x|y = 1)} \frac{p(y = 0)}{p(y = 1)}}
\end{aligned}
$$

代入先验条件

$$
\begin{aligned}
p(y = 1|x)
&= \frac{1}{1 + \frac{\frac{1}{(2\pi)^{\frac n2} |\Sigma|^{\frac12}} \exp(-\frac12 (x - \mu_0)^T \Sigma^{-1} (x - \mu_0))}{\frac{1}{(2\pi)^{\frac n2} |\Sigma|^{\frac12}} \exp(-\frac12(x - \mu_1)^T \Sigma^{-1} (x - \mu_1))} \frac{1 - \phi}{\phi}} \\
&= \frac{1}{1 + \exp(-\frac12 (x - \mu_0)^T \Sigma^{-1} (x - \mu_0) + \frac12 (x - \mu_1)^T \Sigma^{-1} (x - \mu_1)) \frac{1 - \phi}{\phi}} \\
&= \frac{1}{1 + \exp(-\frac12 (x - \mu_0)^T \Sigma^{-1} (x - \mu_0) + \frac12 (x - \mu_1)^T \Sigma^{-1} (x - \mu_1) + \log \frac{1 - \phi}{\phi})}
\end{aligned}
$$

考虑式中 $\exp$ 内部的内容：

$$
\begin{aligned}
&-\frac12 (x - \mu_0)^T \Sigma^{-1} (x - \mu_0) + \frac12 (x - \mu_1)^T \Sigma^{-1} (x - \mu_1) + \log \frac{1-\phi}{\phi} \\
&= -\frac12 (x^T \Sigma^{-1} (\mu_1 - \mu_0) + (\mu_1 - \mu_0)^T \Sigma^{-1} x + \mu_0^T \Sigma^{-1} \mu_0 - \mu_1^T \Sigma^{-1} \mu_1) + \log \frac{1-\phi}{\phi} \\
&= -\frac12 (2 (\mu_1-\mu_0)^T \Sigma^{-1} x + \mu_0^T \Sigma^{-1} \mu_0 - \mu_1^T \Sigma^{-1} \mu_1) + \log \frac{1 - \phi}{\phi} \\
&= -(\mu_1 - \mu_0)^T \Sigma^{-1} x - \frac12 (\mu_0^T \Sigma^{-1} \mu_0 - \mu_1^T \Sigma^{-1} \mu_1) + \log \frac{1 - \phi}{\phi}
\end{aligned}
$$

令

$$
\begin{aligned}
\theta^T &= (\mu_1 - \mu_0)^T \Sigma^{-1} \\
\theta_0 &= \frac12 (\mu_0^T \Sigma^{-1} \mu_0 - \mu_1^T \Sigma^{-1} \mu_1) - \log \frac{1 - \phi} {\phi}
\end{aligned}
$$

代入，化简贝叶斯公式，可得：

$$
\begin{aligned}
p(y = 1|x) = \frac{1}{1 + \exp(-\theta^T x - \theta_0)}
\end{aligned}
$$

可以发现，高斯判别分析的后验分布就是逻辑回归，只不过 $\theta$ 参数的计算形式更为复杂。这也说明逻辑回归具有普遍性。

### 4. 高斯判别分析的决策边界

已知高斯判别分析的后验分布之后，我们就能够求出决策边界，其中决策边界的条件为：

$$
\begin{aligned}
p(y = 0|x) = p(y = 1|x) = 0.5
\end{aligned}
$$

由此可得决策边界方程为:

$$
\begin{aligned}
\theta^T x + \theta_0 = 0
\end{aligned}
$$

### 结语

在这一篇博客中，我们详细分析了高斯判别分析的工作原理，通过公式推导的方法，给出了各个参数的计算方法，以及证明了它与逻辑回归之间隐含的关系。

在使用中，高斯判别分析在样本近似符合正态分布时的效果会更优，不过逻辑回归更具有普适性，并且计算过程更为简单，所以在使用时还需要灵活选择。
